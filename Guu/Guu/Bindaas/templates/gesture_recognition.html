{% extends 'base.html' %}

{% block content %}
<div class="container mt-5">
    <h2 class="mb-4">Gesture Recognition</h2>
    <div class="row">
        <div class="col-md-8 mx-auto">
            <div class="card">
                <div class="card-body">
                    <div class="text-center mb-4">
                        <video id="video" width="640" height="480" autoplay></video>
                        <canvas id="canvas" width="640" height="480" style="display: none;"></canvas>
                    </div>
                    <div class="text-center">
                        <button id="startButton" class="btn btn-primary">Start Camera</button>
                        <button id="stopButton" class="btn btn-danger" disabled>Stop Camera</button>
                    </div>
                    <div class="mt-3">
                        <p id="gesture-output" class="text-center">Detected Gesture: None</p>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/handpose"></script>

<script>
let video = document.getElementById('video');
let canvas = document.getElementById('canvas');
let ctx = canvas.getContext('2d');
let model;
let stream;

async function setupCamera() {
    stream = await navigator.mediaDevices.getUserMedia({
        video: { width: 640, height: 480 },
        audio: false,
    });
    video.srcObject = stream;
    return new Promise((resolve) => {
        video.onloadedmetadata = () => {
            resolve(video);
        };
    });
}

async function loadHandposeModel() {
    model = await handpose.load();
}

async function startDetection() {
    try {
        await setupCamera();
        await loadHandposeModel();
        detectHands();
    } catch (error) {
        console.error('Error:', error);
    }
}

async function detectHands() {
    ctx.drawImage(video, 0, 0, 640, 480);
    const predictions = await model.estimateHands(video);
    
    if (predictions.length > 0) {
        const gesture = interpretGesture(predictions[0].landmarks);
        document.getElementById('gesture-output').textContent = `Detected Gesture: ${gesture}`;
    }
    
    if (video.srcObject) {
        requestAnimationFrame(detectHands);
    }
}

function interpretGesture(landmarks) {
    // Simple gesture detection logic - can be expanded
    const thumb = landmarks[4];
    const indexFinger = landmarks[8];
    
    const distance = Math.sqrt(
        Math.pow(thumb[0] - indexFinger[0], 2) +
        Math.pow(thumb[1] - indexFinger[1], 2)
    );
    
    return distance < 50 ? "Pinch" : "Open Hand";
}

document.getElementById('startButton').addEventListener('click', async () => {
    await startDetection();
    document.getElementById('startButton').disabled = true;
    document.getElementById('stopButton').disabled = false;
});

document.getElementById('stopButton').addEventListener('click', () => {
    if (stream) {
        stream.getTracks().forEach(track => track.stop());
        video.srcObject = null;
    }
    document.getElementById('startButton').disabled = false;
    document.getElementById('stopButton').disabled = true;
    document.getElementById('gesture-output').textContent = 'Detected Gesture: None';
});
</script>
{% endblock %} 